# -*- coding: utf-8 -*-
"""credit model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LgquDXJZPe6zu5UZzdru-5si9Zs19u_t
"""

# importer les packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
import pickle

#Lire la base de donnees
df=pd.read_csv('/content/train_u6lujuX_CVtuZ9i.csv')
df

pd.set_option('display.max_rows',df.shape[0]+1)

df

pd.set_option('display.max_rows',10)

df

#voir les valeurs manquantes
df.info()



df.isnull().sum().sort_values(ascending=False)

df.describe()

df.describe(include='O')

#renseigner les valeurs manquantes
cat_data=[]
num_data=[]
for i,c in enumerate(df.dtypes):
  if c==object:
     cat_data.append(df.iloc[:,i])
  else:
    num_data.append(df.iloc[:,i])
cat_data=pd.DataFrame(cat_data).transpose()
num_data=pd.DataFrame(num_data).transpose()

# Remplacer les valeurs manquantes par la valeur la plus fréquente dans chaque colonne catégorielle
cat_data = cat_data.apply(lambda x: x.fillna(x.value_counts().index[0]))

# Vérifier s'il reste des valeurs manquantes
cat_data.isnull().sum().any()

cat_data['Married'].value_counts()

# pour les variables num on les remplace par la val precedente de la mm colonne
num_data.fillna(method='bfill',inplace=True)
num_data.isnull().sum().any()

num_data

#Transformer le cononne target
target_value={'Y':1,'N':0}
target=cat_data['Loan_Status']
cat_data.drop('Loan_Status',axis=1,inplace=True)
target=target.map(target_value)
target

#Remplacer les val categoriques par des val num
le=LabelEncoder()
for i in cat_data:
  cat_data[i]=le.fit_transform(cat_data[i])
cat_data

# Supprimer loan_id
cat_data.drop('Loan_ID',axis=1,inplace=True)

#Concatener cat_Data et num_data et specifier la colonne target
x=pd.concat([cat_data,num_data],axis=1)
y=target

y

# on commence par la varieble target
target.value_counts()

#la base de données utilisée pour EDA
df=pd.concat([cat_data,num_data,target],axis=1)

# Créer la figure et la taille du graphique
plt.figure(figsize=(8,6))

# Créer le graphique avec sns.countplot
# Palette spécifie les couleurs à utiliser
sns.countplot(x=target, palette=['blue', 'red'])

# Calculer les pourcentages pour chaque catégorie
value_counts = target.value_counts()
yes = value_counts.iloc[0] / len(target)  # Première catégorie
no = value_counts.iloc[1] / len(target)   # Deuxième catégorie

# Afficher les pourcentages
print(f'Le pourcentage des crédits accordés est: {yes * 100:.2f}%')
print(f'Le pourcentage des crédits non accordés est: {no * 100:.2f}%')

# Afficher le graphique
plt.show()

#Credit history
# Créer le FacetGrid
grid = sns.FacetGrid(df, col='Loan_Status', height=3.2, aspect=1.6)

# Mapper les countplots sur chaque sous-graphe
grid.map(sns.countplot, 'Credit_History')

# Afficher le graphique
plt.show()

#sexe
grid = sns.FacetGrid(df, col='Loan_Status', height=3.2, aspect=1.6)
grid.map(sns.countplot, 'Gender')

#situation fam
grid = sns.FacetGrid(df, col='Loan_Status', height=3.2, aspect=1.6)
grid.map(sns.countplot, 'Married')

#ecart relativement faible

#revenue de demandeur
plt.scatter(df['ApplicantIncome'],df['Loan_Status'])

plt.scatter(df['CoapplicantIncome'],df['Loan_Status'])

df.groupby('Loan_Status').median() #pour ne pas avoir l'impact des val adherentes

# Diviser la base de données en une base de données test et une d'entraînement
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

for train_index, test_index in sss.split(x, y):
    x_train, x_test = x.iloc[train_index], x.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

print('x_train taille:', x_train.shape)
print('x_test taille:', x_test.shape)
print('y_train taille:', y_train.shape)
print('y_test taille:', y_test.shape)

#on va appliquer trois algorithmes logistic regression, KMN, DecisionTree
models={
    'LogisticRegression': LogisticRegression(random_state=42),
    'KNeighborsClassifier': KNeighborsClassifier(),
    'Decision tree Classifier': DecisionTreeClassifier(max_depth=1, random_state=42)
}
# La fonction de précision
def accu(y_true,y_pred,retu=False):
  acc=accuracy_score(y_true,y_pred)
  if retu:
    return acc
  else:
    print(f'la precision du modèle est: {acc}')
# c la fonction dapplication des modeles
def train_test_eval(models,x_train,y_train,x_test,y_test):
  for name,models in models.items():
    print(name,':')
    models.fit(x_train,y_train)
    accu(y_test,models.predict(x_test))
    print('-*30')

train_test_eval(models,x_train,y_train,x_test,y_test)

x_2=x[['Credit_History','Married','CoapplicantIncome',]]

# Diviser la base de données en une base de données test et une d'entraînement
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

for train_index, test_index in sss.split(x_2, y):
    x_train, x_test = x_2.iloc[train_index], x_2.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

print('x_train taille:', x_train.shape)
print('x_test taille:', x_test.shape)
print('y_train taille:', y_train.shape)
print('y_test taille:', y_test.shape)

train_test_eval(models,x_train,y_train,x_test,y_test)

#appliquer la regression logistique sur notre base de donnees
# Entraîner le modèle avec les données d'entraînement
Classifier = LogisticRegression()
Classifier.fit(x_2,y)  # Utilise x_train et y_train pour l'entraînement

# enregister le modele
pickle.dump(Classifier,open('model.pkl','wb'))